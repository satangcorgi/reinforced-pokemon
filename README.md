# Reinforced Pok√©mon üß†‚öîÔ∏è

Q-learning agent that battles on Pok√©mon Showdown using poke-env. The bot learns from turn-level rewards (damage, type effectiveness, smart switches, status, immunities) and saves a persistent Q-table so training can continue across runs. It also exports reward plots (line, histogram, box plot) to track progress.

# Repo contents
	‚Ä¢	pokemon.py ‚Äì the RL agent, training loop, and plotting
	‚Ä¢	my_team.txt ‚Äì your team (Showdown ‚ÄúExport‚Äù text)
	‚Ä¢	opponent_team.txt ‚Äì the opponent‚Äôs team (same format)
	‚Ä¢	type_effectiveness.txt ‚Äì JSON map for type multipliers (optional; empty is ok)
	‚Ä¢	requirements.txt ‚Äì Python deps
	‚Ä¢	(generated) line_plot_rewards.png, histogram_rewards.png, boxplot_rewards.png
	‚Ä¢	(generated) q_table.pkl ‚Äì learned Q-table (do not commit this file)

‚∏ª

# 1) Prereqs
	‚Ä¢	Python 3.10+ (3.12 works)
	‚Ä¢	Node.js (to run a local Showdown server)
	‚Ä¢	Git (optional, for cloning)

Install Python deps

(recommended) create a venv
python3 -m venv .venv
source .venv/bin/activate          # Windows: .venv\Scripts\activate

pip install --upgrade pip
pip install -r requirements.txt


‚∏ª

# 2) Start a local Pok√©mon Showdown server

The bot connects to ws://localhost:8000/showdown/websocket.

git clone https://github.com/smogon/pokemon-showdown.git
cd pokemon-showdown
npm ci           # install exact deps
node server      # starts on http://localhost:8000

Mac Gatekeeper tip: If macOS blocks running node scripts, run:

xattr -dr com.apple.quarantine pokemon-showdown



Want a different host/port? Update the server_config line in pokemon.py.

‚∏ª

# 3) Add teams

Open Showdown ‚Üí Teambuilder ‚Üí Your team ‚Üí Export and paste into:
	‚Ä¢	my_team.txt (your bot‚Äôs team)
	‚Ä¢	opponent_team.txt (the fixed opponent team)

The default format in pokemon.py is gen8ou. Make sure your teams are legal there (or change the battle_format in the __main__ block).

‚∏ª

# 4) (Optional) Type effectiveness

type_effectiveness.txt can be empty {} and the bot will still run.
If you want custom multipliers (e.g., buffing immunities), use JSON like:

{
  "Fire": { "Grass": 2, "Water": 0.5, "Rock": 0.5, "Ground": 1, "Ghost": 1 },
  "Ground": { "Electric": 2, "Flying": 0, "Steel": 2, "Poison": 2 }
}

Strings must match the type names the engine sees.

‚∏ª

# 5) Run training

From the repo folder (with your venv activated and Showdown running):

python pokemon.py

	‚Ä¢	The bot plays a series of battles against a RandomPlayer opponent.
	‚Ä¢	After each run you‚Äôll get:
	‚Ä¢	q_table.pkl ‚Äì the saved Q-table (persistent knowledge)
	‚Ä¢	line_plot_rewards.png ‚Äì reward per battle
	‚Ä¢	histogram_rewards.png ‚Äì reward distribution
	‚Ä¢	boxplot_rewards.png ‚Äì reward summary
	‚Ä¢	To continue training, just run again; it will load the existing q_table.pkl.
	‚Ä¢	To reset learning, delete q_table.pkl.

Change total battles by editing n_battles in the run_multiple_battles(...) call at the bottom of pokemon.py.

‚∏ª

# 6) How it learns (reward model)

On each turn/battle end we update Q-values using standard 1-step TD learning:
	‚Ä¢	Damage delta: reward = (damage dealt) ‚àí (damage received)
	‚Ä¢	Type effectiveness:
	‚Ä¢	Super-effective ‚Üí bonus
	‚Ä¢	Not very effective / immune ‚Üí penalty (or bonus if you switched into an immune hit)
	‚Ä¢	Smart switching:
	‚Ä¢	Baseline reward for switching
	‚Ä¢	Extra reward if the new mon has a favorable first move vs the opponent
	‚Ä¢	Big reward if you switched into a move you‚Äôre immune to
	‚Ä¢	Status:
	‚Ä¢	Reward for inflicting a new status
	‚Ä¢	Penalty if your side gets statused

Rewards are kept intentionally simple and local so the bot learns responses, not just memorized sequences.

‚∏ª

# 7) Hyperparameters (where to tweak)

Inside MyCustomPlayer.__init__:
	‚Ä¢	epsilon / epsilon_decay / min_epsilon: exploration ‚Üí exploitation
	‚Ä¢	For training from scratch: epsilon=0.5, epsilon_decay=0.99, min_epsilon=0.1
	‚Ä¢	For evaluation (play ‚Äúfor real‚Äù): set epsilon=0.0 (pure exploitation)
	‚Ä¢	alpha (learning rate): 0.05 is stable; drop to 0.02 if values oscillate
	‚Ä¢	gamma (discount): 0.95 encourages longer-term reward
	‚Ä¢	A small adaptive rule reduces alpha by 10% every 5k battles if win-rate ‚â• 70%

Tip: If you want pure exploitation after training, set epsilon=0.0 and keep your q_table.pkl.

‚∏ª

# 8) Reproducibility & persistence
	‚Ä¢	The Q-table is pickled after every run. Keep it out of git:

echo -e ".venv/\n__pycache__/\n*.pyc\nq_table.pkl\n*.png" >> .gitignore


	‚Ä¢	If you change action_space() later, the loader will pad/truncate rows automatically.

‚∏ª

# 9) Troubleshooting
   ‚Ä¢	‚ÄúConnect call failed ‚Ä¶ 127.0.0.1:8000‚Äù
Start the Showdown server (node server) first, or update server_config to your server‚Äôs URL.
   ‚Ä¢	ModuleNotFoundError: poke_env
Activate the venv and pip install -r requirements.txt.
   ‚Ä¢	Teams not loading / illegal team
Make sure the format in pokemon.py matches your exports and that both teams are valid in that format.

‚∏ª

# 10) Train your own team quickly
	1.	Paste your six Pok√©mon into my_team.txt.
	2.	Paste a target meta or sample squad into opponent_team.txt.
	3.	Start Showdown locally.
	4.	Run python pokemon.py.
	5.	Repeat. The Q-table gets smarter with each session.

When you‚Äôre happy with the learned policy, set epsilon=0.0 and watch it play its best moves.